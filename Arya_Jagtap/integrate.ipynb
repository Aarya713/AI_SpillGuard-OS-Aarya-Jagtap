{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jXdo91BBSTz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D,\n",
        "                                   concatenate, BatchNormalization, Activation,\n",
        "                                   GlobalAveragePooling2D, Dense, Flatten)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
        "                           jaccard_score, confusion_matrix, classification_report,\n",
        "                           roc_curve, auc)\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Enable memory growth to avoid OOM errors\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OilSpillPipeline:\n",
        "    def __init__(self, input_shape=(256, 256, 3)):\n",
        "        self.input_shape = input_shape\n",
        "        self.classifier = None\n",
        "        self.segmenter = None\n",
        "        self.classifier_metrics = {}\n",
        "        self.segmentation_metrics = {}\n",
        "\n",
        "    def build_classifier(self):\n",
        "        \"\"\"Build a lightweight CNN classifier\"\"\"\n",
        "        inputs = Input(self.input_shape)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        x = Dense(64, activation='relu')(x)\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.classifier = model\n",
        "        return model\n",
        "\n",
        "    def build_segmenter(self, filters=32):\n",
        "        \"\"\"Build a compact U-Net for segmentation\"\"\"\n",
        "        inputs = Input(self.input_shape)\n",
        "\n",
        "        # Encoder\n",
        "        def conv_block(x, filters, dropout_rate=0.0):\n",
        "            x = Conv2D(filters, 3, padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "            x = Conv2D(filters, 3, padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "            if dropout_rate > 0:\n",
        "                x = Dropout(dropout_rate)(x)\n",
        "            return x\n",
        "\n",
        "        # Contracting path\n",
        "        c1 = conv_block(inputs, filters)\n",
        "        p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "        c2 = conv_block(p1, filters*2)\n",
        "        p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "        c3 = conv_block(p2, filters*4)\n",
        "        p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "        # Bottleneck\n",
        "        c4 = conv_block(p3, filters*8, dropout_rate=0.3)\n",
        "\n",
        "        # Expanding path\n",
        "        u1 = UpSampling2D((2, 2))(c4)\n",
        "        u1 = Conv2D(filters*4, 2, padding='same')(u1)\n",
        "        u1 = concatenate([u1, c3])\n",
        "        c5 = conv_block(u1, filters*4)\n",
        "\n",
        "        u2 = UpSampling2D((2, 2))(c5)\n",
        "        u2 = Conv2D(filters*2, 2, padding='same')(u2)\n",
        "        u2 = concatenate([u2, c2])\n",
        "        c6 = conv_block(u2, filters*2)\n",
        "\n",
        "        u3 = UpSampling2D((2, 2))(c6)\n",
        "        u3 = Conv2D(filters, 2, padding='same')(u3)\n",
        "        u3 = concatenate([u3, c1])\n",
        "        c7 = conv_block(u3, filters)\n",
        "\n",
        "        outputs = Conv2D(1, 1, activation='sigmoid')(c7)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.segmenter = model\n",
        "        return model\n",
        "\n",
        "    def dice_coefficient(self, y_true, y_pred, smooth=1e-6):\n",
        "        \"\"\"Dice coefficient metric for segmentation evaluation\"\"\"\n",
        "        y_true_f = tf.reshape(y_true, [-1])\n",
        "        y_pred_f = tf.reshape(y_pred, [-1])\n",
        "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "        return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "    def dice_loss(self, y_true, y_pred):\n",
        "        \"\"\"Dice loss for segmentation\"\"\"\n",
        "        return 1 - self.dice_coefficient(y_true, y_pred)\n",
        "\n",
        "    def bce_dice_loss(self, y_true, y_pred):\n",
        "        \"\"\"Combined BCE and Dice loss\"\"\"\n",
        "        y_true_flat = tf.reshape(y_true, [-1])\n",
        "        y_pred_flat = tf.reshape(y_pred, [-1])\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true_flat, y_pred_flat)\n",
        "        dice = self.dice_loss(y_true, y_pred)\n",
        "        return bce + dice\n",
        "\n",
        "    def compile_models(self, classifier_lr=1e-4, segmenter_lr=1e-4):\n",
        "        \"\"\"Compile both models\"\"\"\n",
        "        if self.classifier is None or self.segmenter is None:\n",
        "            raise ValueError(\"Models not built yet. Please build models first.\")\n",
        "\n",
        "        # Compile classifier\n",
        "        self.classifier.compile(\n",
        "            optimizer=Adam(learning_rate=classifier_lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', Precision(), Recall(), AUC()]\n",
        "        )\n",
        "\n",
        "        # Compile segmenter\n",
        "        self.segmenter.compile(\n",
        "            optimizer=Adam(learning_rate=segmenter_lr),\n",
        "            loss=self.bce_dice_loss,\n",
        "            metrics=['accuracy', self.dice_coefficient]\n",
        "        )"
      ],
      "metadata": {
        "id": "OzswdmmfIvy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def train_classifier(self, X_train, y_train, X_val, y_val, epochs=20, batch_size=32, class_weight=None):\n",
        "        \"\"\"Train the classifier model\"\"\"\n",
        "        if self.classifier is None:\n",
        "            raise ValueError(\"Classifier not built yet.\")\n",
        "\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                'best_classifier.h5',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                mode='max',\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=8,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=4,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.classifier.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_segmenter(self, X_train, y_train, X_val, y_val, epochs=15, batch_size=8):\n",
        "        \"\"\"Train the segmentation model\"\"\"\n",
        "        if self.segmenter is None:\n",
        "            raise ValueError(\"Segmenter not built yet.\")\n",
        "\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                'best_segmenter.h5',\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                mode='min',\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=6,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=3,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.segmenter.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history"
      ],
      "metadata": {
        "id": "fn31FELeI2RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def evaluate_classifier(self, X_test, y_test, threshold=0.5):\n",
        "        \"\"\"Evaluate the classifier model\"\"\"\n",
        "        if self.classifier is None:\n",
        "            raise ValueError(\"Classifier not trained yet.\")\n",
        "\n",
        "        # Keras evaluation\n",
        "        results = self.classifier.evaluate(X_test, y_test, verbose=0)\n",
        "        metrics = {\n",
        "            'loss': results[0],\n",
        "            'accuracy': results[1],\n",
        "            'precision': results[2],\n",
        "            'recall': results[3],\n",
        "            'auc': results[4]\n",
        "        }\n",
        "\n",
        "        # Additional metrics\n",
        "        y_pred_probs = self.classifier.predict(X_test, verbose=0)\n",
        "        y_pred = (y_pred_probs > threshold).astype(int)\n",
        "\n",
        "        metrics['f1_score'] = f1_score(y_test, y_pred)\n",
        "        metrics['confusion_matrix'] = confusion_matrix(y_test, y_pred).tolist()\n",
        "\n",
        "        # ROC curve data\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
        "        metrics['roc_curve'] = {'fpr': fpr.tolist(), 'tpr': tpr.tolist()}\n",
        "        metrics['roc_auc'] = auc(fpr, tpr)\n",
        "\n",
        "        self.classifier_metrics = metrics\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_segmenter(self, X_test, y_test, batch_size=8):\n",
        "        \"\"\"Evaluate the segmentation model\"\"\"\n",
        "        if self.segmenter is None:\n",
        "            raise ValueError(\"Segmenter not trained yet.\")\n",
        "\n",
        "        # Evaluate in batches\n",
        "        num_batches = int(np.ceil(len(X_test) / batch_size))\n",
        "        metrics_accum = {\n",
        "            'loss': 0,\n",
        "            'accuracy': 0,\n",
        "            'dice_coefficient': 0\n",
        "        }\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, len(X_test))\n",
        "\n",
        "            X_batch = X_test[start_idx:end_idx]\n",
        "            y_batch = y_test[start_idx:end_idx]\n",
        "\n",
        "            results = self.segmenter.evaluate(X_batch, y_batch, verbose=0)\n",
        "            metrics_accum['loss'] += results[0] * len(X_batch)\n",
        "            metrics_accum['accuracy'] += results[1] * len(X_batch)\n",
        "            metrics_accum['dice_coefficient'] += results[2] * len(X_batch)\n",
        "\n",
        "        # Average metrics\n",
        "        metrics = {\n",
        "            'loss': float(metrics_accum['loss'] / len(X_test)),\n",
        "            'accuracy': float(metrics_accum['accuracy'] / len(X_test)),\n",
        "            'dice_coefficient': float(metrics_accum['dice_coefficient'] / len(X_test))\n",
        "        }\n",
        "\n",
        "        # Calculate additional metrics on a subset\n",
        "        subset_size = min(50, len(X_test))\n",
        "        indices = np.random.choice(len(X_test), subset_size, replace=False)\n",
        "        X_subset = X_test[indices]\n",
        "        y_subset = y_test[indices]\n",
        "\n",
        "        y_pred = self.segmenter.predict(X_subset, verbose=0)\n",
        "        y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
        "\n",
        "        y_test_flat = y_subset.squeeze().flatten()\n",
        "        y_pred_flat = y_pred_binary.flatten()\n",
        "\n",
        "        metrics['f1_score'] = float(f1_score(y_test_flat, y_pred_flat, zero_division=1))\n",
        "        metrics['jaccard_score'] = float(jaccard_score(y_test_flat, y_pred_flat, zero_division=1))\n",
        "        metrics['confusion_matrix'] = confusion_matrix(y_test_flat, y_pred_flat).tolist()\n",
        "\n",
        "        self.segmentation_metrics = metrics\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "qdcvbByTI6Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def predict_pipeline(self, images, classifier_threshold=0.5, segmenter_threshold=0.5):\n",
        "        \"\"\"\n",
        "        End-to-end prediction: first classify, then segment if oil spill detected\n",
        "        Returns: classifications, segmentations, confidence scores\n",
        "        \"\"\"\n",
        "        if self.classifier is None or self.segmenter is None:\n",
        "            raise ValueError(\"Models not trained yet.\")\n",
        "\n",
        "        # Add batch dimension if needed\n",
        "        if len(images.shape) == 3:\n",
        "            images = np.expand_dims(images, axis=0)\n",
        "\n",
        "        # Classify images\n",
        "        class_probs = self.classifier.predict(images, verbose=0).flatten()\n",
        "        classifications = (class_probs > classifier_threshold).astype(int)\n",
        "\n",
        "        # Only segment images classified as having oil spills\n",
        "        segmentations = np.zeros((len(images), *self.input_shape[:2]), dtype=np.uint8)\n",
        "\n",
        "        oil_spill_indices = np.where(classifications == 1)[0]\n",
        "        if len(oil_spill_indices) > 0:\n",
        "            oil_images = images[oil_spill_indices]\n",
        "            oil_masks = self.segmenter.predict(oil_images, verbose=0)\n",
        "            oil_masks_binary = (oil_masks.squeeze() > segmenter_threshold).astype(np.uint8)\n",
        "\n",
        "            for i, idx in enumerate(oil_spill_indices):\n",
        "                segmentations[idx] = oil_masks_binary[i]\n",
        "\n",
        "        return classifications, segmentations, class_probs\n",
        "\n",
        "    def visualize_results(self, images, true_classifications=None, true_masks=None,\n",
        "                         num_samples=5, save_path=None):\n",
        "        \"\"\"\n",
        "        Visualize pipeline results with optional ground truth comparison\n",
        "        \"\"\"\n",
        "        classifications, segmentations, confidences = self.predict_pipeline(images)\n",
        "\n",
        "        # Select random samples\n",
        "        if len(images) > num_samples:\n",
        "            indices = np.random.choice(len(images), num_samples, replace=False)\n",
        "        else:\n",
        "            indices = range(len(images))\n",
        "\n",
        "        fig, axes = plt.subplots(len(indices), 4, figsize=(16, 4 * len(indices)))\n",
        "        if len(indices) == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            # Original image\n",
        "            axes[i, 0].imshow(images[idx])\n",
        "            axes[i, 0].set_title('Original Image')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # Classification result\n",
        "            class_text = f\"Oil Spill: {classifications[idx]} (Conf: {confidences[idx]:.3f})\"\n",
        "            axes[i, 1].imshow(images[idx])\n",
        "            axes[i, 1].set_title(class_text, color='green' if classifications[idx] else 'red')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            # Segmentation result (if applicable)\n",
        "            if classifications[idx] == 1:\n",
        "                axes[i, 2].imshow(segmentations[idx], cmap='jet')\n",
        "                axes[i, 2].set_title('Predicted Oil Spill')\n",
        "            else:\n",
        "                axes[i, 2].imshow(np.zeros_like(segmentations[idx]), cmap='gray')\n",
        "                axes[i, 2].set_title('No Oil Spill Predicted')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "            # Ground truth (if available)\n",
        "            if true_classifications is not None and true_masks is not None:\n",
        "                gt_class = true_classifications[idx]\n",
        "                axes[i, 3].imshow(images[idx])\n",
        "\n",
        "                if gt_class == 1:\n",
        "                    # Show ground truth mask\n",
        "                    axes[i, 3].imshow(true_masks[idx].squeeze(), alpha=0.5, cmap='jet')\n",
        "                    title_text = f\"Ground Truth: Oil Spill\"\n",
        "                else:\n",
        "                    title_text = f\"Ground Truth: No Oil Spill\"\n",
        "\n",
        "                axes[i, 3].set_title(title_text)\n",
        "                axes[i, 3].axis('off')\n",
        "            else:\n",
        "                axes[i, 3].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def save_models(self, classifier_path='oil_spill_classifier.h5',\n",
        "                   segmenter_path='oil_spill_segmenter.h5'):\n",
        "        \"\"\"Save both models\"\"\"\n",
        "        if self.classifier is not None:\n",
        "            self.classifier.save(classifier_path)\n",
        "            print(f\"Classifier saved to {classifier_path}\")\n",
        "\n",
        "        if self.segmenter is not None:\n",
        "            self.segmenter.save(segmenter_path, custom_objects={\n",
        "                'dice_coefficient': self.dice_coefficient,\n",
        "                'dice_loss': self.dice_loss,\n",
        "                'bce_dice_loss': self.bce_dice_loss\n",
        "            })\n",
        "            print(f\"Segmenter saved to {segmenter_path}\")\n",
        "\n",
        "    def load_models(self, classifier_path='oil_spill_classifier.h5',\n",
        "                   segmenter_path='oil_spill_segmenter.h5'):\n",
        "        \"\"\"Load pre-trained models\"\"\"\n",
        "        self.classifier = load_model(classifier_path)\n",
        "        print(f\"Classifier loaded from {classifier_path}\")\n",
        "\n",
        "        self.segmenter = load_model(segmenter_path, custom_objects={\n",
        "            'dice_coefficient': self.dice_coefficient,\n",
        "            'dice_loss': self.dice_loss,\n",
        "            'bce_dice_loss': self.bce_dice_loss\n",
        "        })\n",
        "        print(f\"Segmenter loaded from {segmenter_path}\")"
      ],
      "metadata": {
        "id": "JobBSAguI2ch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}