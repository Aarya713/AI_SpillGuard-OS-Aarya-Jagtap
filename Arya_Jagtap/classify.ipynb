{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D,\n",
        "                                   concatenate, BatchNormalization, Activation,\n",
        "                                   GlobalAveragePooling2D, Dense, Flatten, Reshape)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Precision, Recall, MeanIoU, AUC\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
        "                           jaccard_score, confusion_matrix, classification_report,\n",
        "                           roc_curve, auc, RocCurveDisplay)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "PREPROCESSED_DATA_PATH = '/content/drive/MyDrive/preprocessed_dataset'\n",
        "\n",
        "def load_preprocessed_data(data_dir, split_name):\n",
        "    \"\"\"\n",
        "    Load preprocessed images and masks from directory\n",
        "    \"\"\"\n",
        "    images_dir = os.path.join(data_dir, split_name, 'images')\n",
        "    masks_dir = os.path.join(data_dir, split_name, 'masks')\n",
        "\n",
        "    if not os.path.exists(images_dir) or not os.path.exists(masks_dir):\n",
        "        raise ValueError(f\"Preprocessed data not found for {split_name}\")\n",
        "\n",
        "    # Get sorted lists of files\n",
        "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.png')])\n",
        "    mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith('.png')])\n",
        "\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for img_file, mask_file in tqdm(zip(image_files, mask_files), desc=f\"Loading {split_name} data\", total=len(image_files)):\n",
        "        # Load image\n",
        "        img_path = os.path.join(images_dir, img_file)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = image.astype(np.float32) / 255.0  # Normalize\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(masks_dir, mask_file)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask.astype(np.float32) / 255.0  # Normalize to 0-1\n",
        "\n",
        "        # Add channel dimension to mask\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        images.append(image)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "def prepare_classification_data(images, masks, threshold=0.01):\n",
        "    \"\"\"\n",
        "    Prepare classification data from segmentation data\n",
        "    Returns images and labels (1 if oil spill present, 0 otherwise)\n",
        "    \"\"\"\n",
        "    # Calculate the percentage of oil pixels in each mask\n",
        "    oil_percentages = np.array([np.sum(mask) / (mask.size) for mask in masks])\n",
        "\n",
        "    # Create binary labels: 1 if oil percentage > threshold, else 0\n",
        "    labels = (oil_percentages > threshold).astype(np.int32)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def calculate_class_weights_binary(y_train):\n",
        "    \"\"\"\n",
        "    Calculate class weights for binary classification\n",
        "    \"\"\"\n",
        "    class_counts = np.bincount(y_train)\n",
        "    total = np.sum(class_counts)\n",
        "\n",
        "    # Handle case where one class might be missing\n",
        "    if len(class_counts) == 1:\n",
        "        if class_counts[0] > 0:\n",
        "            class_counts = np.array([class_counts[0], 0])\n",
        "        else:\n",
        "            class_counts = np.array([0, class_counts[0]])\n",
        "\n",
        "    # Calculate weights (inverse of frequency)\n",
        "    class_weights = {\n",
        "        0: float(total / (2 * class_counts[0])) if class_counts[0] > 0 else 1.0,\n",
        "        1: float(total / (2 * class_counts[1])) if class_counts[1] > 0 else 1.0\n",
        "    }\n",
        "\n",
        "    print(f\"Class counts: No Oil Spill={class_counts[0]}, Oil Spill={class_counts[1]}\")\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "    return class_weights\n",
        "\n",
        "class UNetFeatureExtractor:\n",
        "    def __init__(self, input_shape=(256, 256, 3)):\n",
        "        self.input_shape = input_shape\n",
        "        self.model = None\n",
        "        self.feature_extractor = None\n",
        "\n",
        "    def build_unet(self, filters=64, dropout_rate=0.5, batch_norm=True):\n",
        "        \"\"\"\n",
        "        Build U-Net model for feature extraction\n",
        "        \"\"\"\n",
        "        inputs = Input(self.input_shape)\n",
        "\n",
        "        # Encoder (Contracting Path)\n",
        "        def conv_block(x, filters, dropout_rate=0.0, batch_norm=True):\n",
        "            x = Conv2D(filters, 3, padding='same')(x)\n",
        "            if batch_norm:\n",
        "                x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "            x = Conv2D(filters, 3, padding='same')(x)\n",
        "            if batch_norm:\n",
        "                x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "            if dropout_rate > 0:\n",
        "                x = Dropout(dropout_rate)(x)\n",
        "            return x\n",
        "\n",
        "        # Encoder\n",
        "        c1 = conv_block(inputs, filters, 0.0, batch_norm)\n",
        "        p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "        c2 = conv_block(p1, filters*2, 0.0, batch_norm)\n",
        "        p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "        c3 = conv_block(p2, filters*4, 0.0, batch_norm)\n",
        "        p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "        c4 = conv_block(p3, filters*8, dropout_rate, batch_norm)\n",
        "        p4 = MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "        # Bottleneck\n",
        "        c5 = conv_block(p4, filters*16, dropout_rate, batch_norm)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=inputs, outputs=c5)\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def extract_features_batch(self, images, batch_size=32):\n",
        "        \"\"\"\n",
        "        Extract features using U-Net encoder with batch processing\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built yet. Please build the model first.\")\n",
        "\n",
        "        features = []\n",
        "        num_batches = int(np.ceil(len(images) / batch_size))\n",
        "\n",
        "        for i in tqdm(range(num_batches), desc=\"Extracting features\"):\n",
        "            batch_start = i * batch_size\n",
        "            batch_end = min((i + 1) * batch_size, len(images))\n",
        "            batch_images = images[batch_start:batch_end]\n",
        "\n",
        "            batch_features = self.model.predict(batch_images, verbose=0)\n",
        "            features.append(batch_features)\n",
        "\n",
        "        return np.vstack(features)\n",
        "\n",
        "class OilSpillClassifier:\n",
        "    def __init__(self, input_shape=(16, 16, 1024)):  # Default shape from U-Net bottleneck\n",
        "        self.input_shape = input_shape\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.metrics = {}\n",
        "\n",
        "    def build_classifier(self):\n",
        "        \"\"\"\n",
        "        Build classifier on top of U-Net features\n",
        "        \"\"\"\n",
        "        inputs = Input(self.input_shape)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = GlobalAveragePooling2D()(inputs)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        # Additional dense layers\n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, learning_rate=1e-4):\n",
        "        \"\"\"\n",
        "        Compile the classification model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built yet. Please build the model first.\")\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', Precision(), Recall(), AUC()]\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32,\n",
        "              callbacks=None, class_weight=None):\n",
        "        \"\"\"\n",
        "        Train the classification model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built yet. Please build the model first.\")\n",
        "\n",
        "        # Default callbacks\n",
        "        if callbacks is None:\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(\n",
        "                    '/content/best_classifier_model.h5',\n",
        "                    monitor='val_accuracy',\n",
        "                    save_best_only=True,\n",
        "                    mode='max',\n",
        "                    verbose=1\n",
        "                ),\n",
        "                EarlyStopping(\n",
        "                    monitor='val_accuracy',\n",
        "                    patience=10,\n",
        "                    restore_best_weights=True,\n",
        "                    verbose=1\n",
        "                ),\n",
        "                ReduceLROnPlateau(\n",
        "                    monitor='val_loss',\n",
        "                    factor=0.2,\n",
        "                    patience=5,\n",
        "                    min_lr=1e-7,\n",
        "                    verbose=1\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        # Train the model\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def evaluate(self, X_test, y_test, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Evaluate the classification model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet. Please train the model first.\")\n",
        "\n",
        "        # Keras evaluation\n",
        "        results = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "        metrics = {\n",
        "            'loss': results[0],\n",
        "            'accuracy': results[1],\n",
        "            'precision': results[2],\n",
        "            'recall': results[3],\n",
        "            'auc': results[4]\n",
        "        }\n",
        "\n",
        "        # Additional metrics using sklearn\n",
        "        y_pred_probs = self.model.predict(X_test, verbose=0)\n",
        "        y_pred = (y_pred_probs > threshold).astype(int)\n",
        "\n",
        "        metrics['f1_score'] = f1_score(y_test, y_pred)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        metrics['confusion_matrix'] = cm.tolist()\n",
        "        if cm.size == 4:\n",
        "            metrics['tn'], metrics['fp'], metrics['fn'], metrics['tp'] = cm.ravel()\n",
        "        else:\n",
        "            metrics['tn'], metrics['fp'], metrics['fn'], metrics['tp'] = 0, 0, 0, 0\n",
        "\n",
        "        # Classification report\n",
        "        metrics['classification_report'] = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # ROC curve data\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
        "        metrics['roc_curve'] = {'fpr': fpr.tolist(), 'tpr': tpr.tolist()}\n",
        "        metrics['roc_auc'] = auc(fpr, tpr)\n",
        "\n",
        "        self.metrics = metrics\n",
        "        return metrics\n",
        "\n",
        "    def predict(self, features, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Predict class probabilities for features\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet. Please train the model first.\")\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.model.predict(features, verbose=0)\n",
        "        binary_predictions = (predictions > threshold).astype(int)\n",
        "\n",
        "        return predictions.squeeze(), binary_predictions.squeeze()\n",
        "\n",
        "    def plot_training_history(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Plot training history\n",
        "        \"\"\"\n",
        "        if self.history is None:\n",
        "            raise ValueError(\"No training history available. Please train the model first.\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Plot loss\n",
        "        axes[0, 0].plot(self.history.history['loss'], label='Training Loss')\n",
        "        axes[0, 0].plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "        axes[0, 0].set_title('Model Loss')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # Plot accuracy\n",
        "        axes[0, 1].plot(self.history.history['accuracy'], label='Training Accuracy')\n",
        "        axes[0, 1].plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        axes[0, 1].set_title('Model Accuracy')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "\n",
        "        # Plot precision\n",
        "        axes[0, 2].plot(self.history.history['precision'], label='Training Precision')\n",
        "        axes[0, 2].plot(self.history.history['val_precision'], label='Validation Precision')\n",
        "        axes[0, 2].set_title('Model Precision')\n",
        "        axes[0, 2].set_ylabel('Precision')\n",
        "        axes[0, 2].set_xlabel('Epoch')\n",
        "        axes[0, 2].legend()\n",
        "        axes[0, 2].grid(True)\n",
        "\n",
        "        # Plot recall\n",
        "        axes[1, 0].plot(self.history.history['recall'], label='Training Recall')\n",
        "        axes[1, 0].plot(self.history.history['val_recall'], label='Validation Recall')\n",
        "        axes[1, 0].set_title('Model Recall')\n",
        "        axes[1, 0].set_ylabel('Recall')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        # Plot AUC\n",
        "        axes[1, 1].plot(self.history.history['auc'], label='Training AUC')\n",
        "        axes[1, 1].plot(self.history.history['val_auc'], label='Validation AUC')\n",
        "        axes[1, 1].set_title('Model AUC')\n",
        "        axes[1, 1].set_ylabel('AUC')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True)\n",
        "\n",
        "        # Remove empty subplot\n",
        "        axes[1, 2].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix\n",
        "        \"\"\"\n",
        "        if not self.metrics or 'confusion_matrix' not in self.metrics:\n",
        "            raise ValueError(\"No evaluation metrics available. Please evaluate the model first.\")\n",
        "\n",
        "        cm = np.array(self.metrics['confusion_matrix'])\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['No Oil Spill', 'Oil Spill'],\n",
        "                   yticklabels=['No Oil Spill', 'Oil Spill'])\n",
        "        plt.title('Confusion Matrix - Classification')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_roc_curve(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Plot ROC curve\n",
        "        \"\"\"\n",
        "        if not self.metrics or 'roc_curve' not in self.metrics:\n",
        "            raise ValueError(\"No ROC curve data available. Please evaluate the model first.\")\n",
        "\n",
        "        fpr = self.metrics['roc_curve']['fpr']\n",
        "        tpr = self.metrics['roc_curve']['tpr']\n",
        "        roc_auc = self.metrics['roc_auc']\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def save_model(self, model_path='/content/oil_spill_classifier.h5'):\n",
        "        \"\"\"\n",
        "        Save the trained model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet. Please train the model first.\")\n",
        "\n",
        "        self.model.save(model_path)\n",
        "        print(f\"Classifier model saved to {model_path}\")\n",
        "\n",
        "    def load_model(self, model_path='/content/oil_spill_classifier.h5'):\n",
        "        \"\"\"\n",
        "        Load a trained model\n",
        "        \"\"\"\n",
        "        self.model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"Classifier model loaded from {model_path}\")\n",
        "\n",
        "    def generate_report(self, save_path='/content/classification_report.json'):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive classification report\n",
        "        \"\"\"\n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'model_architecture': 'U-Net Feature-based Classifier',\n",
        "            'input_shape': self.input_shape,\n",
        "            'metrics': self.metrics,\n",
        "            'training_history': {\n",
        "                'final_training_loss': self.history.history['loss'][-1] if self.history else None,\n",
        "                'final_validation_loss': self.history.history['val_loss'][-1] if self.history else None,\n",
        "                'final_training_accuracy': self.history.history['accuracy'][-1] if self.history else None,\n",
        "                'final_validation_accuracy': self.history.history['val_accuracy'][-1] if self.history else None,\n",
        "                'epochs_trained': len(self.history.history['loss']) if self.history else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "\n",
        "        print(f\"Classification report saved to {save_path}\")\n",
        "        return report\n",
        "\n",
        "def train_classification_pipeline():\n",
        "    \"\"\"\n",
        "    Complete pipeline for training oil spill classification using U-Net features\n",
        "    \"\"\"\n",
        "    # Load preprocessed data\n",
        "    data_dir = PREPROCESSED_DATA_PATH\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    X_train_seg, y_train_seg = load_preprocessed_data(data_dir, 'train')\n",
        "    X_val_seg, y_val_seg = load_preprocessed_data(data_dir, 'val')\n",
        "    X_test_seg, y_test_seg = load_preprocessed_data(data_dir, 'test')\n",
        "\n",
        "    print(f\"Segmentation data shapes:\")\n",
        "    print(f\"Train: {X_train_seg.shape}, {y_train_seg.shape}\")\n",
        "    print(f\"Val: {X_val_seg.shape}, {y_val_seg.shape}\")\n",
        "    print(f\"Test: {X_test_seg.shape}, {y_test_seg.shape}\")\n",
        "\n",
        "    # Prepare classification data\n",
        "    print(\"\\nPreparing classification data...\")\n",
        "    X_train_cls, y_train_cls = prepare_classification_data(X_train_seg, y_train_seg)\n",
        "    X_val_cls, y_val_cls = prepare_classification_data(X_val_seg, y_val_seg)\n",
        "    X_test_cls, y_test_cls = prepare_classification_data(X_test_seg, y_test_seg)\n",
        "\n",
        "    print(f\"Classification data shapes:\")\n",
        "    print(f\"Train: {X_train_cls.shape}, {y_train_cls.shape}\")\n",
        "    print(f\"Val: {X_val_cls.shape}, {y_val_cls.shape}\")\n",
        "    print(f\"Test: {X_test_cls.shape}, {y_test_cls.shape}\")\n",
        "\n",
        "    # Calculate class weights\n",
        "    cls_class_weights = calculate_class_weights_binary(y_train_cls)\n",
        "\n",
        "    # Build U-Net feature extractor\n",
        "    print(\"\\nBuilding U-Net feature extractor...\")\n",
        "    feature_extractor = UNetFeatureExtractor(input_shape=(256, 256, 3))\n",
        "    unet_model = feature_extractor.build_unet(filters=64, dropout_rate=0.3, batch_norm=True)\n",
        "\n",
        "    # Extract features with batch processing\n",
        "    print(\"\\nExtracting features from training data...\")\n",
        "    X_train_features = feature_extractor.extract_features_batch(X_train_cls, batch_size=32)\n",
        "    X_val_features = feature_extractor.extract_features_batch(X_val_cls, batch_size=32)\n",
        "    X_test_features = feature_extractor.extract_features_batch(X_test_cls, batch_size=32)\n",
        "\n",
        "    print(f\"Feature shapes:\")\n",
        "    print(f\"Train features: {X_train_features.shape}\")\n",
        "    print(f\"Val features: {X_val_features.shape}\")\n",
        "    print(f\"Test features: {X_test_features.shape}\")\n",
        "\n",
        "    # Build and train classifier\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING CLASSIFICATION MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    classifier = OilSpillClassifier(input_shape=X_train_features.shape[1:])\n",
        "    cls_model = classifier.build_classifier()\n",
        "    cls_model.summary()\n",
        "\n",
        "    classifier.compile_model(learning_rate=1e-4)\n",
        "\n",
        "    cls_history = classifier.train(\n",
        "        X_train_features, y_train_cls, X_val_features, y_val_cls,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        class_weight=cls_class_weights\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    cls_metrics = classifier.evaluate(X_test_features, y_test_cls)\n",
        "    print(\"\\nClassification Evaluation Metrics:\")\n",
        "    for metric, value in cls_metrics.items():\n",
        "        if metric not in ['confusion_matrix', 'classification_report', 'roc_curve']:\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_cls,\n",
        "                               (classifier.model.predict(X_test_features) > 0.5).astype(int)))\n",
        "\n",
        "    # Generate reports and visualizations\n",
        "    cls_report = classifier.generate_report('/content/classification_report.json')\n",
        "\n",
        "    # Save models\n",
        "    classifier.save_model('/content/oil_spill_classifier.h5')\n",
        "\n",
        "    # Plot results\n",
        "    classifier.plot_training_history('/content/classification_training_history.png')\n",
        "    classifier.plot_confusion_matrix('/content/classification_confusion_matrix.png')\n",
        "    classifier.plot_roc_curve('/content/roc_curve.png')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSIFICATION TRAINING COMPLETE!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return classifier, cls_metrics, feature_extractor\n",
        "\n",
        "# Alternative: Simple CNN classifier (much faster)\n",
        "def train_simple_cnn_classifier():\n",
        "    \"\"\"\n",
        "    Train a simple CNN classifier for faster results\n",
        "    \"\"\"\n",
        "    # Load preprocessed data\n",
        "    data_dir = PREPROCESSED_DATA_PATH\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    X_train_seg, y_train_seg = load_preprocessed_data(data_dir, 'train')\n",
        "    X_val_seg, y_val_seg = load_preprocessed_data(data_dir, 'val')\n",
        "    X_test_seg, y_test_seg = load_preprocessed_data(data_dir, 'test')\n",
        "\n",
        "    # Prepare classification data\n",
        "    X_train_cls, y_train_cls = prepare_classification_data(X_train_seg, y_train_seg)\n",
        "    X_val_cls, y_val_cls = prepare_classification_data(X_val_seg, y_val_seg)\n",
        "    X_test_cls, y_test_cls = prepare_classification_data(X_test_seg, y_test_seg)\n",
        "\n",
        "    # Calculate class weights\n",
        "    cls_class_weights = calculate_class_weights_binary(y_train_cls)\n",
        "\n",
        "    # Build simple CNN model\n",
        "    print(\"\\nBuilding simple CNN classifier...\")\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(256, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', Precision(), Recall(), AUC()]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nTraining simple CNN classifier...\")\n",
        "    history = model.fit(\n",
        "        X_train_cls, y_train_cls,\n",
        "        batch_size=32,\n",
        "        epochs=20,\n",
        "        validation_data=(X_val_cls, y_val_cls),\n",
        "        class_weight=cls_class_weights,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = model.evaluate(X_test_cls, y_test_cls, verbose=0)\n",
        "    print(f\"\\nTest accuracy: {results[1]:.4f}\")\n",
        "    print(f\"Test precision: {results[2]:.4f}\")\n",
        "    print(f\"Test recall: {results[3]:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    model.save('/content/simple_cnn_classifier.h5')\n",
        "    print(\"Simple CNN classifier saved!\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Run the classification pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Choose classification approach:\")\n",
        "    print(\"1. U-Net Feature-based Classifier (More accurate but slower)\")\n",
        "    print(\"2. Simple CNN Classifier (Faster but less accurate)\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1 or 2): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # Train U-Net feature-based classifier\n",
        "        classifier, cls_metrics, feature_extractor = train_classification_pipeline()\n",
        "    elif choice == \"2\":\n",
        "        # Train simple CNN classifier\n",
        "        model, history = train_simple_cnn_classifier()\n",
        "    else:\n",
        "        print(\"Invalid choice. Running U-Net feature-based classifier...\")\n",
        "        classifier, cls_metrics, feature_extractor = train_classification_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JvodwwkPXL0L",
        "outputId": "6c137182-b37e-4baf-aecb-0bc28b8d5754"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Choose classification approach:\n",
            "1. U-Net Feature-based Classifier (More accurate but slower)\n",
            "2. Simple CNN Classifier (Faster but less accurate)\n",
            "Loading data...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 1666/1666 [06:40<00:00,  4.16it/s]\n",
            "Loading val data: 100%|██████████| 203/203 [00:42<00:00,  4.75it/s]\n",
            "Loading test data: 100%|██████████| 254/254 [00:53<00:00,  4.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class counts: No Oil Spill=370, Oil Spill=1296\n",
            "Class weights: {0: 2.2513513513513512, 1: 0.6427469135802469}\n",
            "\n",
            "Building simple CNN classifier...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50176</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,690,624</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50176\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m25,690,624\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,210,625</span> (99.99 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,210,625\u001b[0m (99.99 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,210,625</span> (99.99 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,210,625\u001b[0m (99.99 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training simple CNN classifier...\n",
            "Epoch 1/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 4s/step - accuracy: 0.5750 - auc: 0.5890 - loss: 0.6869 - precision: 0.8152 - recall: 0.5744 - val_accuracy: 0.8670 - val_auc: 0.9128 - val_loss: 0.3762 - val_precision: 0.9096 - val_recall: 0.9264\n",
            "Epoch 2/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 4s/step - accuracy: 0.7484 - auc: 0.8136 - loss: 0.5314 - precision: 0.9111 - recall: 0.7492 - val_accuracy: 0.6355 - val_auc: 0.9266 - val_loss: 0.5976 - val_precision: 0.9890 - val_recall: 0.5521\n",
            "Epoch 3/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 4s/step - accuracy: 0.7398 - auc: 0.8620 - loss: 0.4592 - precision: 0.9486 - recall: 0.7076 - val_accuracy: 0.7143 - val_auc: 0.9426 - val_loss: 0.5024 - val_precision: 0.9907 - val_recall: 0.6503\n",
            "Epoch 4/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 4s/step - accuracy: 0.7913 - auc: 0.9088 - loss: 0.3948 - precision: 0.9490 - recall: 0.7738 - val_accuracy: 0.9458 - val_auc: 0.9810 - val_loss: 0.1638 - val_precision: 0.9419 - val_recall: 0.9939\n",
            "Epoch 5/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 4s/step - accuracy: 0.8775 - auc: 0.9359 - loss: 0.3217 - precision: 0.9480 - recall: 0.8943 - val_accuracy: 0.9163 - val_auc: 0.9827 - val_loss: 0.2086 - val_precision: 0.9932 - val_recall: 0.9018\n",
            "Epoch 6/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 4s/step - accuracy: 0.9138 - auc: 0.9657 - loss: 0.2421 - precision: 0.9771 - recall: 0.9116 - val_accuracy: 0.9458 - val_auc: 0.9871 - val_loss: 0.1606 - val_precision: 0.9935 - val_recall: 0.9387\n",
            "Epoch 7/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 4s/step - accuracy: 0.9006 - auc: 0.9645 - loss: 0.2376 - precision: 0.9765 - recall: 0.8945 - val_accuracy: 0.9409 - val_auc: 0.9847 - val_loss: 0.1587 - val_precision: 0.9935 - val_recall: 0.9325\n",
            "Epoch 8/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 4s/step - accuracy: 0.9394 - auc: 0.9810 - loss: 0.1766 - precision: 0.9819 - recall: 0.9399 - val_accuracy: 0.9310 - val_auc: 0.9870 - val_loss: 0.1331 - val_precision: 0.9745 - val_recall: 0.9387\n",
            "Epoch 9/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 4s/step - accuracy: 0.9369 - auc: 0.9829 - loss: 0.1759 - precision: 0.9746 - recall: 0.9433 - val_accuracy: 0.8670 - val_auc: 0.9878 - val_loss: 0.3216 - val_precision: 1.0000 - val_recall: 0.8344\n",
            "Epoch 10/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 4s/step - accuracy: 0.9191 - auc: 0.9809 - loss: 0.1818 - precision: 0.9834 - recall: 0.9116 - val_accuracy: 0.9458 - val_auc: 0.9880 - val_loss: 0.1046 - val_precision: 0.9578 - val_recall: 0.9755\n",
            "Epoch 11/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 4s/step - accuracy: 0.9362 - auc: 0.9848 - loss: 0.1639 - precision: 0.9716 - recall: 0.9477 - val_accuracy: 0.9507 - val_auc: 0.9894 - val_loss: 0.1466 - val_precision: 0.9935 - val_recall: 0.9448\n",
            "Epoch 12/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 4s/step - accuracy: 0.9551 - auc: 0.9894 - loss: 0.1341 - precision: 0.9859 - recall: 0.9556 - val_accuracy: 0.9064 - val_auc: 0.9906 - val_loss: 0.1979 - val_precision: 0.9932 - val_recall: 0.8896\n",
            "Epoch 13/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 4s/step - accuracy: 0.9544 - auc: 0.9921 - loss: 0.1233 - precision: 0.9878 - recall: 0.9524 - val_accuracy: 0.9458 - val_auc: 0.9773 - val_loss: 0.1277 - val_precision: 0.9750 - val_recall: 0.9571\n",
            "Epoch 14/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 4s/step - accuracy: 0.9183 - auc: 0.9709 - loss: 0.2326 - precision: 0.9744 - recall: 0.9195 - val_accuracy: 0.9507 - val_auc: 0.9916 - val_loss: 0.1588 - val_precision: 0.9935 - val_recall: 0.9448\n",
            "Epoch 15/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 4s/step - accuracy: 0.9453 - auc: 0.9892 - loss: 0.1325 - precision: 0.9850 - recall: 0.9452 - val_accuracy: 0.9557 - val_auc: 0.9908 - val_loss: 0.1494 - val_precision: 0.9936 - val_recall: 0.9509\n",
            "Epoch 16/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 4s/step - accuracy: 0.9401 - auc: 0.9857 - loss: 0.1511 - precision: 0.9851 - recall: 0.9380 - val_accuracy: 0.9212 - val_auc: 0.9883 - val_loss: 0.2122 - val_precision: 0.9933 - val_recall: 0.9080\n",
            "Epoch 17/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 3s/step - accuracy: 0.9646 - auc: 0.9927 - loss: 0.1098 - precision: 0.9882 - recall: 0.9659 - val_accuracy: 0.9212 - val_auc: 0.9906 - val_loss: 0.1896 - val_precision: 0.9933 - val_recall: 0.9080\n",
            "Epoch 18/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 4s/step - accuracy: 0.9670 - auc: 0.9947 - loss: 0.0883 - precision: 0.9961 - recall: 0.9616 - val_accuracy: 0.9507 - val_auc: 0.9917 - val_loss: 0.1543 - val_precision: 0.9935 - val_recall: 0.9448\n",
            "Epoch 19/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 3s/step - accuracy: 0.9741 - auc: 0.9960 - loss: 0.0819 - precision: 0.9924 - recall: 0.9738 - val_accuracy: 0.9261 - val_auc: 0.9788 - val_loss: 0.2510 - val_precision: 0.9933 - val_recall: 0.9141\n",
            "Epoch 20/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 4s/step - accuracy: 0.9662 - auc: 0.9965 - loss: 0.0765 - precision: 0.9912 - recall: 0.9655 - val_accuracy: 0.9655 - val_auc: 0.9821 - val_loss: 0.1436 - val_precision: 0.9937 - val_recall: 0.9632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 0.9724\n",
            "Test precision: 0.9949\n",
            "Test recall: 0.9700\n",
            "Simple CNN classifier saved!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}